import torch.nn as nn
import torch

def init_weights(m):
    if isinstance(m, nn.Conv2d):
        std = 0.1
        torch.nn.init.trunc_normal_(m.weight, mean=0.0, std=0.1, a=-2*std, b=2*std)

class backbone(nn.Module):
    def __init__(self, in_channels, bias=False):
        super().__init__()
        self.act   = nn.ReLU(inplace=True)

        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=4, kernel_size=3, stride=1, padding=3 // 2, bias=bias)
        self.bn1   = nn.BatchNorm2d(num_features=4)

        self.conv2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=3 // 2, bias=bias)
        self.bn2   = nn.BatchNorm2d(num_features=8)

        self.conv3 = nn.Conv2d(in_channels=12, out_channels=4, kernel_size=3, stride=1, padding=3 // 2, bias=bias)
        self.bn3   = nn.BatchNorm2d(num_features=4)

        self.conv4 = nn.Conv2d(in_channels=16, out_channels=4, kernel_size=3, stride=1, padding=3 // 2, bias=bias)
        self.bn4   = nn.BatchNorm2d(num_features=4)

        self.conv5 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=2, stride=2, bias=bias)
        self.bn5   = nn.BatchNorm2d(num_features=8)

        self.conv6 = nn.Conv2d(in_channels=8, out_channels=4, kernel_size=3, stride=1, padding=3 // 2, bias=bias)
        self.bn6   = nn.BatchNorm2d(num_features=4)

        self.conv7 = nn.Conv2d(in_channels=12, out_channels=4, kernel_size=3, stride=1, padding=3 // 2, bias=bias)
        self.bn7   = nn.BatchNorm2d(num_features=4)

        self.conv8 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=3 // 2, bias=bias)
        self.bn8   = nn.BatchNorm2d(num_features=8)

        self.conv9 = nn.Conv2d(in_channels=8, out_channels=4, kernel_size=3, stride=1, padding=3 // 2,bias=bias)
        # self.conv9 = nn.Conv2d(in_channels=8, out_channels=4, kernel_size=1, stride=1, padding=0,bias=bias)
        self.bn9   = nn.BatchNorm2d(num_features=4)

        self.conv10 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=3 // 2, bias=bias)
        self.bn10   = nn.BatchNorm2d(num_features=8)

        self.conv11 = nn.Conv2d(in_channels=12, out_channels=4, kernel_size=3, stride=1, padding=3 // 2, bias=bias)
        self.bn11   = nn.BatchNorm2d(num_features=4)

        self.conv12 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=3 // 2, bias=bias)
        self.bn12   = nn.BatchNorm2d(num_features=8)

        self.conv_up = nn.ConvTranspose2d(in_channels=8, out_channels=4, kernel_size=2, stride=2 ,bias=bias)
        self.bn_up   = nn.BatchNorm2d(num_features=4)

    def forward(self, x):

        o11 = self.act(self.bn1(self.conv1(x)))
        o12 = self.act(self.bn2(self.conv2(o11)))
        o13 = self.act(self.bn3(self.conv3(torch.cat((o11, o12), dim=1))))
        o14 = self.act(self.bn4(self.conv4(torch.cat((o11, o12, o13), dim=1))))

        o21 = self.act(self.bn5(self.conv5(o14)))
        o22 = self.act(self.bn6(self.conv6(o21)))
        o23 = self.act(self.bn7(self.conv7(torch.cat((o21, o22), dim=1))))
        o24 = self.act(self.bn8(self.conv8(torch.cat((o21, o22, o23), dim=1))))

        o31 = self.act(self.bn9(self.conv9(o24)))
        o32 = self.act(self.bn10(self.conv10(o31)))
        o33 = self.act(self.bn11(self.conv11(torch.cat((o31, o32), dim=1))))
        o34 = self.act(self.bn12(self.conv12(torch.cat((o31, o32, o33), dim=1))))

        out = self.bn_up(self.conv_up(o34))
        out = self.act(torch.add(out, o11))
        return out

class DenseDeconNet(nn.Module):
    def __init__(self, in_channels, scale_factor=1) -> None:
        super().__init__()
        self.scale_facotr = scale_factor

        self.backbone = backbone(in_channels=in_channels, bias=False)
        self.conv_out = nn.Conv2d(in_channels=4, out_channels=in_channels, kernel_size=3, stride=1, padding=3 // 2, bias=False)
        self.bn_out   = nn.BatchNorm2d(num_features=in_channels)
        self.act_out  = nn.LeakyReLU(inplace=True, negative_slope=0.2)
    
        self.backbone.apply(init_weights)
        init_weights(self.conv_out)

    def forward(self, x):
        if self.scale_facotr > 1:
            x = nn.functional.interpolate(input=x, scale_factor=self.scale_facotr, mode='bicubic')

        fea = self.backbone(x)
        out = self.act_out(self.bn_out(self.conv_out(fea)))
        return out

if __name__ == '__main__':
    x = torch.ones(size=(2, 3, 128, 128))
    model = DenseDeconNet(in_channels=3, scale_factor=4)
    o = model(x)
    print(o.shape)
